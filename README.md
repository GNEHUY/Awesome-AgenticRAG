<div align="center">
  <!-- <p align="center"> -->
  <h1 align="center"><strong>Awesome-AgenticRAG</strong></h1>
</div>

🔬 列举一些关于AgenticRAG的系列文章，以2025年开始，包括Search-O1，Search-R1

- [2025.01] [[Search-o1]](https://arxiv.org/abs/2501.05366) Search-o1: Agentic Search-Enhanced Large Reasoning Models 增强具有类似O1推理模式的LRMs的自主检索能力，使模型在推理过程中能动态检索外部知识，从而提高推理的准确性和可靠性 [![[code]](https://img.shields.io/github/stars/sunnynexus/Search-o1)](https://github.com/sunnynexus/Search-o1)
- [2025.02] [[O1 Embedder]](https://arxiv.org/abs/2502.07555) O1 Embedder: Let Retrievers Think Before Action 已经有很多训练LLM作为Embedder的工作，如何让Embedder在检索目标文档之前生成对输入查询有用的thoughts，类似于一个推理的过程？[![[code]](https://img.shields.io/github/stars/RuiranYan/o1embedder)](https://github.com/RuiranYan/o1embedder)
- [2025.03] [DeepRetrieval](https://arxiv.org/abs/2503.00223) DeepRetrieval: Hacking Real Search Engines and Retrievers with Large Language Models via Reinforcement Learning 与前面基于答案匹配度作为奖励信号不同(前面主要是RAG的QA任务)，该工作主要聚焦在检索任务，以检索指标作为奖励信号，LLM通过查询增强的方式，补充原始查询的语义，然后进行检索 [![[code]](https://img.shields.io/github/stars/pat-jj/DeepRetrieval)](https://github.com/pat-jj/DeepRetrieval)
- [2025.03] [[Search-R1]](https://arxiv.org/abs/2503.09516) Search-R1: Training LLMs to Reason and Leverage Search Engines with Reinforcement Learning 收到R1的启发，将强化学习扩展到RAG场景，将搜索引擎建模为强化学习环境的一部分，使LLM能通过试错自主学习；仅用最终答案正确性作为奖励信号，创新检索内容掩码 [![[code]](https://img.shields.io/github/stars/PeterGriffinJin/Search-R1)](https://github.com/PeterGriffinJin/Search-R1)
- [2025.03] [[R1-Searcher]](https://arxiv.org/abs/2503.05592) R1-Searcher: Incentivizing the Search Capability in LLMs via Reinforcement Learning 与Search-R1类似，不过采用的是基于两阶段RL框架，通过自主调用外部搜索工具增强LLM的回答能力，无过程奖励或蒸馏。仅依赖最终奖励。 [![[code]](https://img.shields.io/github/stars/RUCAIBox/R1-Searcher)](https://github.com/RUCAIBox/R1-Searcher)
- [2025.03] [[ReSearch]](https://arxiv.org/abs/2503.19470) ReSearch: Learning to Reason with Search for LLMs via Reinforcement Learning 基本和Search-R1一样，不同点在于考虑了格式奖励，同时用的是F1 score [![[code]](https://img.shields.io/github/stars/Agent-RL/ReSearch)](https://github.com/Agent-RL/ReSearch)
- [2025.03] [[ReAgent]](https://arxiv.org/abs/2503.06951) ReAgent: Reversible Multi-Agent Reasoning for  Knowledge-Enhanced Multi-Hop QA 通过引入多智能体可逆回溯推理机制，解决了多跳问答中错误积累和不可纠正的问题。 [![[code]](https://img.shields.io/github/stars/astridesa/ReAgent)](https://github.com/astridesa/ReAgent)
- [2025.04] [[DeepResearcher]](https://arxiv.org/abs/2504.03160) DeepResearcher: Scaling Deep Research via Reinforcement Learning in Real-World Environments 是现有搜索代理在实际环境中扩展困难，通过强化学习在真实环境中扩展深度研究能力，缺少在真实网络环境中，应对环境动态性，不可预测性，噪声、搜索网页质量差异和内容格式问题的强大Agent框架，不仅有Search还有Browse [![[code]](https://img.shields.io/github/stars/GAIR-NLP/DeepResearcher)](https://github.com/GAIR-NLP/DeepResearcher)
- [2025.05] [[AutoRefine]](https://arxiv.org/abs/2505.11277) Search and Refine During Think: Facilitating Knowledge Refinement for Improved Retrieval-Augmented Reasoning Search-R1检索到的文档往往包含无关内容，可能影响到模型有效利用新的知识。可以考虑边检索，边精炼的方式，使模型在检索过程中自我进化。同时精炼过程提供奖励，避免仅结果奖励 [![[code]](https://img.shields.io/github/stars/syr-cn/AutoRefine)](https://github.com/syr-cn/AutoRefine)
- [2025.05] [[IKEA]](https://arxiv.org/abs/2505.07596) IKEA: Reinforced Internal-External Knowledge Synergistic Reasoning for Efficient Adaptive Search Agent 解决现有搜索代理过度依赖外部搜索、未充分利用内部知识的问题，提出强化学习的内外部知识协同推理代理，识别知识边界，优先使用内部知识，减少冗余检索和知识冲突 [![[code]](https://img.shields.io/github/stars/hzy312/knowledge-r1)](https://github.com/hzy312/knowledge-r1)
- [2025.05] [[ZeroSearch]](https://arxiv.org/abs/2505.04588) ZeroSearch: Incentivize the Search Capability of LLMs without Searching 解决RL训练搜索代理时面临的文档质量不可控和API成本高昂两大挑战，无需真实搜索，直接用LLM模拟搜索引擎，引入课程学习策略，在降低88%成本的同时性能超过依赖真实搜索的方法 [![[code]](https://img.shields.io/github/stars/Alibaba-NLP/ZeroSearch)](https://github.com/Alibaba-NLP/ZeroSearch)
- [2025.05] [[s3]](https://arxiv.org/abs/2505.14146) s3: You Don't Need That Much Data to Train a Search Agent via RL 解决现有方法要么优化检索指标忽略下游效用，要么端到端训练导致搜索与生成纠缠的问题，提出轻量级框架解耦搜索器和生成器，仅用2.4k训练样本实现强大性能，提出Gain Beyond RAG奖励 [![[code]](https://img.shields.io/github/stars/pat-jj/s3)](https://github.com/pat-jj/s3)
- [2025.05] [[StepSearch]](https://arxiv.org/abs/2505.15107) StepSearch: Igniting LLMs Search Ability via Step-Wise Proximal Policy Optimization 目前Search-R1等现有方法因依赖稀疏全局奖励而缺乏对中间搜索过程细粒度监督的问题，通过引入基于信息增益和冗余惩罚的token级别步骤奖励机制（StePPO），解决了其在复杂多跳问答中缺乏中间查询和多步检索细粒度监督的问题。需要做数据增强得到Golden轨迹 [![[code]](https://img.shields.io/github/stars/Zillwang/StepSearch)](https://github.com/Zillwang/StepSearch)
- [2025.05] [[R1-Searcher++]](https://arxiv.org/abs/2505.17005) R1-Searcher++: Incentivizing the Dynamic Knowledge Acquisition of LLMs via Reinforcement Learning 作为R1-Searcher的增强版，解决如何更好地利用内部和外部知识的问题，采用两阶段策略，引入内部知识利用奖励机制和记忆机制，实现动态知识获取 [![[code]](https://img.shields.io/github/stars/RUCAIBox/R1-Searcher-plus)](https://github.com/RUCAIBox/R1-Searcher-plus)
- [2025.05] [[Search Wisely β-GRPO]](https://arxiv.org/abs/2505.17281) Search Wisely: Mitigating Sub-optimal Agentic Searches By Reducing Uncertainty 解决代理搜索中存在的不确定性导致次优搜索行为（搜索不足or冗余搜索）的问题，通过减少不确定性来缓解次优的代理搜索，提高搜索效率和质量 [![[code]](https://img.shields.io/github/stars/mianzhang/Search-R1)](https://github.com/mianzhang/Search-R1)
- [2025.05] [[LeTS]](https://arxiv.org/abs/2505.17447) LeTS: Learning to Think-and-Search via Process-and-Outcome Reward Hybridization 解决了Search-R1/ReSearch等结果监督RL方法因忽略中间步骤而导致的冗余搜索与无关搜索问题，通过设计基于规则的过程级奖励模块（包括 惩罚同一轨迹内重复检索相同文档的行为 和 利用组内优秀轨迹指导弱轨迹，解决无关搜索问题）并用过程奖励动态调整结果奖励的优势值实现过程-结果奖励混合 [![[code]](https://img.shields.io/github/stars/Cheungki/LeTS)](https://github.com/Cheungki/LeTS)
- [2025.05] [[EvolveSearch]](https://arxiv.org/abs/2505.22501) EvolveSearch: An Iterative Self-Evolving Search Agent 解决当前搜索代理需要外部人工标注推理轨迹的问题，提出迭代自进化框架，协同结合RL与SFT，无需外部人工标注即可提升网络搜索能力，实现自我进化
- [2025.06] [[R-Search]](https://arxiv.org/abs/2506.04185) R-Search: Empowering LLM Reasoning with Search via Multi-Reward Reinforcement Learning 通过引入多阶段混合奖励机制（答案质量、跨模型证据质量、格式正确性）和证据整合模块，解决了Search-R1中检索时机与真实需求不对齐、推理-搜索交互深度受限的问题，使LLM能够动态决定何时检索并从全局视角提炼关键证据，从而优化整个推理-搜索交互轨迹 [![[code]](https://img.shields.io/github/stars/QingFei1/R-Search)](https://github.com/QingFei1/R-Search)
- [2025.08] [[Self-Search RL]](https://arxiv.org/abs/2508.10874) SSRL: Self-Search Reinforcement Learning 研究LLM作为RL任务模拟器的潜力，训练LLM直接作为搜索引擎，减少对外部搜索引擎的昂贵交互依赖，通过结构化提示和重复采样量化LLM的内在搜索能力，增强自我搜索能力 [![[code]](https://img.shields.io/github/stars/TsinghuaC3I/SSRL)](https://github.com/TsinghuaC3I/SSRL)
- [2025.08] [[ASearcher]](https://arxiv.org/abs/2508.07976) Beyond Ten Turns: Unlocking Long-Horizon Agentic Search with Large-Scale Asynchronous RL 解决长视野搜索任务的挑战，通过大规模异步强化学习解锁长视野代理搜索能力，支持超过十轮以上的复杂搜索交互 [![[code]](https://img.shields.io/github/stars/inclusionAI/ASearcher)](https://github.com/inclusionAI/ASearcher)
- [2025.08] [[Atom-Searcher]](https://arxiv.org/abs/2508.12800) Atom-Searcher: Enhancing Agentic Deep Research via Fine-Grained Atomic Thought Reward 深度研究依赖结果奖励 RL，存在梯度冲突与奖励稀疏，训练低效且难收敛。提出“原子思维”范式，把推理拆成细粒度功能单元，并设计原子思维奖励 ATR 对其逐段打分；再用课程式聚合策略先重过程 ATR、后重结果奖励，平滑优化路径。Atom-Searcher 框架 = 原子思维分解 + ATR 细粒度引导 + 课程式混合奖励，无需额外标注即可在七项基准上稳定超越 SOTA，推理更可解释、测试算力可伸缩。[![[code]](https://img.shields.io/github/stars/antgroup/Research-Venus)](https://github.com/antgroup/Research-Venus)
- [2025.09] [[EviNote-RAG]](https://arxiv.org/abs/2509.00877) EviNote-RAG: Enhancing RAG Models via Answer-Supportive Evidence Notes 针对 RAG 信号噪声低、多跳误差累积两大痛点，提出“检索→笔记→回答”新流程：先让模型把原始文档蒸馏成“支持证据笔记”（SEN），显式标记关键与不确定信息，再用基于蕴涵的 Evidence Quality Reward 保证笔记足以推出答案 [![[code]](https://img.shields.io/github/stars/Da1yuqin/EviNoteRAG)](https://github.com/Da1yuqin/EviNoteRAG)
- [2025.09] [[AceSearcher]](https://arxiv.org/abs/2509.24193) AceSearcher: Bootstrapping Reasoning and Search for LLMs via Reinforced Self-Play 检索增强 LLM 多跳检索弱、推理差，于是让同一模型“左右互搏”：自演分解者+解题者，用监督微调混合搜索推理任务后，再直接用最终答案准确率做强化学习，无需中间标注，10 数据集平均 EM 提升 7.6%，32B 版仅用 DeepSeek-V3 5% 参数就在金融文档推理上打平，1.5/8B 版也常跑赢参数量大 9 倍的现有模型。[![[code]](https://img.shields.io/github/stars/ritaranx/AceSearcher)](https://github.com/ritaranx/AceSearcher)
- [2025.10] [[DeSA]](https://arxiv.org/abs/2510.04695) Beyond Outcome Reward: Decoupling Search and Answering Improves LLM Agents 目前Search-R1主要依赖基于结果的奖励，这隐含了一个关键假设：优化最终答案会自动教会智能体进行有效搜索。作者质疑这一假设，指出结果奖励存在以下根本缺陷：信用分配问题：结果奖励提供的是稀疏、延迟的反馈，无法有效指导中间的搜索行为 行为-结果脱节：没有证据表明好的结果必然来自于有效的搜索过程。导致不搜索，重复搜索，无效搜索。 [![[code]](https://img.shields.io/github/stars/yiding-w/DeSA)](https://github.com/yiding-w/DeSA)
- [2025.10] [[DecEx-RAG]](https://arxiv.org/abs/2510.05691) DecEx-RAG: Boosting Agentic Retrieval-Augmented Generation with Decision and Execution Optimization via Process Supervision 将RAG建模为马尔可夫决策过程，显式解耦决策（终止/检索）与执行（内容质量）两阶段，通过搜索树构建过程监督数据，并利用多轮rollout的聚合奖励动态剪枝冗余分支，将搜索复杂度从指数降为线性。采用SFT+DPO两阶段训练学习最优决策与执行策略 [![[code]](https://img.shields.io/github/stars/sdsxdxl/DecEx-RAG)](https://github.com/sdsxdxl/DecEx-RAG)
- [2025.10] [[HiPRAG]](https://arxiv.org/abs/2510.07794) HiPRAG: Hierarchical Process Rewards for Efficient Agentic Retrieval Augmented Generation 通过分层过程奖励优化RAG智能体搜索决策，将推理轨迹分解为可解析步骤并实时检测冗余/缺失搜索 β-GRPO续作 [![[code]](https://img.shields.io/github/stars/qualidea1217/HiPRAG)](https://github.com/qualidea1217/HiPRAG)
- [2025.10] [[QAgent]](https://arxiv.org/abs/2510.08383) QAgent: A modular Search Agent with Interactive Query Understanding 解决传统RAG难以理解复杂查询、RL训练搜索代理泛化和部署困难的问题，提出模块化搜索代理框架，通过交互式推理和检索优化查询理解，即插即用于复杂系统 [![[code]](https://img.shields.io/github/stars/LivingFutureLab/QAgent)](https://github.com/LivingFutureLab/QAgent)
- [2025.10] [[InfoFlow]](https://arxiv.org/abs/2510.26575) InfoFlow: Reinforcing Search Agent via Reward Density Optimization 解决深度搜索场景中奖励密度低、探索成本高的问题，提出奖励密度优化框架，通过子问题分解、失败引导提示和双代理精炼三方面提高奖励密度和训练效率
- [2025.10] [[Search Self-play]](https://arxiv.org/abs/2510.18821) Search Self-play: Pushing the Frontier of Agent Capability without Supervision 解决无监督情况下如何提升代理能力的问题，通过搜索自我博弈强化学习，让LLM交替提问和解决持续训练自我进化，无需监督即可推动代理能力边界，解决当前训练Agent的RL方法对数据的依赖问题 [![[code]](https://img.shields.io/github/stars/Alibaba-Quark/SSP)](https://github.com/Alibaba-Quark/SSP)
- [2025.10] [[E-GRPO]](https://arxiv.org/abs/2510.24694) Repurposing Synthetic Data for Fine-grained Search Agent Supervision 解决GRPO方法缺乏细粒度监督信号的问题，提出E-GRPO框架，利用合成数据中的实体信息作为细粒度奖励，解决"近失"问题，提升复杂搜索任务性能
- [2025.10] [[GlobalRAG]](https://arxiv.org/abs/2510.20548) GlobalRAG: Enhancing Global Reasoning in Multi-hop Question Answering via Reinforcement Learning 解决多跳QA中缺乏全局规划和不忠实执行的问题，通过强化学习增强全局推理，分解问题为子目标，协调检索与推理，仅使用8k训练数据就实现显著性能提升。需要做数据增强得到Golden轨迹 [![[code]](https://img.shields.io/github/stars/CarnegieBin/GlobalRAG)](https://github.com/CarnegieBin/GlobalRAG)
- [2025.10] [[Interact-RAG]](https://arxiv.org/abs/2510.27566) Interact-RAG: Reason and Interact with the Corpus, Beyond Black-Box Retrieval 现有Agentic RAG方法将检索过程视为黑盒，智能体只能被动查询，限制了复杂任务的信息探索能力。Interact-RAG通过语料库交互引擎赋予智能体细粒度检索控制权：多面检索（语义/精确搜索、加权融合）锚定匹配（实体匹配聚焦关键信息）上下文塑造（动态包含/排除文档、调整检索规模）配合推理增强工作流（全局规划器→自适应推理器→执行器）实现零样本执行和轨迹合成，再通过SFT+RL两阶段训练（GRPO算法）内化策略
- [2025.10] [[MARAG-R1]](https://arxiv.org/abs/2510.27569) MARAG-R1: Beyond Single Retriever via Reinforcement-Learned Multi-Tool Agentic Retrieval 现有RAG系统依赖单一检索器和固定的top-k选择策略，这导致：只能访问语料库的狭窄静态子集；无法获取任务所需的全面外部信息；在需要语料库级推理和跨文档综合的任务上成为主要瓶颈。多工具架构：为LLM配备四种互补的检索工具：语义搜索（广度探索）关键词搜索（精确匹配）文档过滤（基于约束选择）聚合工具（统计综合）两阶段训练：SFT+RL 复合奖励设计：答案奖励 + 文档覆盖率奖励 + 工具探索奖励：平衡探索效率，避免冗余调用
- [2025.11] [[TeaRAG]](https://arxiv.org/abs/2511.05385) TeaRAG: A Token-Efficient Agentic Retrieval-Augmented Generation Framework 现有Agentic RAG方法因过度关注答案准确性而忽视token开销，导致模型过度思考和冗余检索，且训练效率低下。TeaRAG通过两个层面提升token效率：1) 检索压缩：构建知识关联图融合语义检索与三元组图检索，用Personalized PageRank筛选关键信息，以高密度知识三元组替代冗余文本块；2) 推理压缩：提出迭代式过程感知DPO（IP-DPO），设计基于知识匹配的过程奖励函数评估每步的知识充分性并惩罚多余步骤，通过迭代优化生成更简洁的推理路径。[![[code]](https://img.shields.io/github/stars/Applied-Machine-Learning-Lab/TeaRAG)](https://github.com/Applied-Machine-Learning-Lab/TeaRAG)
- [2025.11] [[IterResearch]](https://arxiv.org/abs/2511.07327) IterResearch: Rethinking Long-Horizon Agents via Markovian State Reconstruction 通过马尔可夫状态重建机制（用演进报告替代完整历史上下文）解决了单上下文范式的上下文窒息与噪声污染问题，使智能体在任意探索深度（实验验证至 2048 轮）保持恒定推理能力。提出EAPO（效率感知策略优化）——引入几何折扣奖励激励高效探索，配合自适应下采样实现稳定分布式训练。
- [2025.11] [[Bi-RAR]](https://arxiv.org/abs/2511.09109) Thinking Forward and Backward: Multi-Objective Reinforcement Learning for Retrieval-Augmented Reasoning 现有检索增强推理方法因仅依赖最终答案监督，易导致模型生成冗长低效推理链和幻觉。通过Kolmogorov复杂度理论量化每个推理步骤的双向信息距离——既衡量与答案的接近程度（正向），也评估对问题的契合度（反向），并采用多目标强化学习优化这两个目标：设计级联奖励结构鼓励早期建立正确方向，独立训练正向/反向模型后通过权重插值融合，实现细粒度的步骤级监督，生成更精确简洁的推理过程。
- [2025.12] [[LLDS&MA-GRPO4Search-R1]](https://arxiv.org/abs/2512.04220) On GRPO Collapse in Search-R1: The Lazy Likelihood-Displacement Death Spiral 为了解决再用GRPO训练Search-R1出现突然崩溃的现象，作者发现其核心原因是"懒惰似然位移"（LLD）——优化过程中正确与错误响应的似然度均出现停滞或下降，进而引发自我强化的"LLD死亡螺旋"，导致低置信度响应、梯度膨胀和训练崩溃。提出轻量级似然保持正则化LLDS，通过响应级门控（仅当轨迹总似然下降时激活）和令牌级选择性（仅惩罚导致下降的令牌），精准抑制LLD且最小化对优化的干扰，其变体LLDS-MA掩码答案令牌以鼓励多步推理。🌟
- [2025.12] [[RouteRAG]](https://arxiv.org/abs/2512.09487) RouteRAG: Efficient Retrieval-Augmented Generation from Text and Graph via Reinforcement Learning 解决RAG缺乏自适应能力：图结构或混合检索系统依赖固定流程，无法像文本RAG那样通过强化学习实现多轮动态检索，难以在推理过程中按需补充证据；检索效率问题：图检索虽对多跳推理至关重要，但计算成本远高于文本检索，现有方法无法根据查询需求灵活选择检索方式，导致不必要的开销；通过Search-R1的范式实现，两阶段RL训练[![[code]](https://img.shields.io/github/stars/YucanGuo/RouteRAG)](https://github.com/YucanGuo/RouteRAG)
- [2025.12] [[EKA]](https://arxiv.org/abs/2512.20144) Multi-hop Reasoning via Early Knowledge Alignment 现有迭代RAG系统因模型在缺乏检索语料上下文的情况下分解问题，导致初始推理缺乏信息基础，易产生错误检索和级联错误。为此提出早期知识对齐（EKA），通过在规划阶段前执行首次检索并将结果注入模型，使其后续强化学习优化的迭代过程具备上下文基础 [![[code]](https://img.shields.io/github/stars/yxzwang/EarlyKnowledgeAlignment)](https://github.com/yxzwang/EarlyKnowledgeAlignment) 
- [2025.12] [[Laser]](https://arxiv.org/abs/2512.20458) Laser: Governing Long-Horizon Agentic Search via Structured Protocol and Context Register 基于LLM/LRM的智能体搜索系统面临两个核心瓶颈：自然语言推理的脆弱性和上下文窗口污染溢出，Laser通过符号化动作协议+状态寄存器，将智能体搜索从"自由发挥的自然语言对话"转变为"可解析、可回溯、高效率的结构化程序执行"，从根本上解决了长程推理的稳定性和可扩展性问题。免训练模式：直接通过提示工程驱动（在Qwen3-8b/32b上表现强劲）；RFT微调：使用拒绝采样微调（Rejection Sampling Fine-Tuning），从强模型（DeepSeek-V3.1）收集高质量结构化轨迹进行蒸馏[![[code]](https://img.shields.io/github/stars/ShootingWong/Laser)](https://github.com/ShootingWong/Laser) 
- [2025.12] [[HGMem]](https://arxiv.org/abs/2512.23959) Improving Multi-step RAG with Hypergraph-based Memory for Long-Context Complex Relational Modeling 把多步 RAG 的“工作记忆”从堆叠孤立事实的被动仓库升级为可动态演化的超图：用超边灵活建模 n 阶关系，通过检索-更新-插入-合并循环让记忆不断长出高阶关联骨架，从而给后续推理提供结构化、全局化的知识支撑，显著提升长文本复杂关系与全局理解任务的效果 [![[code]](https://img.shields.io/github/stars/Encyclomen/HGMem)](https://github.com/Encyclomen/HGMem)
- [2026.01] [[ARR]](https://arxiv.org/abs/2601.04651) Adversarial Yet Cooperative: Multi-Perspective Reasoning in Retrieved-Augmented Language Models 对检索增强推理模型单视角局限和训练信号不足的问题，提出“对抗推理RAG”框架：让推理器与验证器互评逻辑，并用过程感知奖励同时优化两者，无需外部打分即可提升多步推理质量
- [2026.01] [[GRACE]](https://arxiv.org/abs/2601.04525) GRACE: Reinforcement Learning for Grounded Response and Abstention under Contextual Evidence 针对RAG“无证据也答、证据不足就编”的双重幻觉，用异构检索器自动生成训练样本，再以多阶段门控奖励强化学习，让模型同时学会证据接地与主动弃权，用1/10标注成本实现准确率与拒答率的新平衡 [![[code]](https://img.shields.io/github/stars/YiboZhao624/Grace)](https://github.com/YiboZhao624/Grace)
- [2026.01] [[SmartSearch]](https://arxiv.org/abs/2601.04888) SmartSearch: Process Reward-Guided Query Refinement for Search Agents 发现 LLM 搜索智能体败在中间查询不准，于是用“过程奖励+双层信用评估”实时给每步查询打分，并只重训劣质查询及其后续轮次；配合“模仿-对齐-泛化”三阶段课程学习，让智能体自优化查询质量，在效率与准确率上全面超越现有基线。[![[code]](https://img.shields.io/github/stars/MYVAE/SmartSearch)](https://github.com/MYVAE/SmartSearch)
- [2026.01] [[PRISMA]](https://arxiv.org/abs/2601.05465) PRISMA: Reinforcement Learning Guided Two-Stage Policy Optimization in Multi-Agent Architecture for Open-Domain Multi-Hop Question Answering 发现端到端 RL 多跳 RAG 会“检索崩溃”找不到桥接证据，又“信用分配弱”易过拟合，于是把系统拆成 Plan-Retrieve-Inspect-Solve-Memoize 五智能体：Inspector提供基于推理的反馈，精炼规划者的分解方案，并强制要求 Solver 进行基于证据的推理，Solver 必须接地；两阶段：GRPO 先分别把 Planner/Solver 训成专家，再用 OARPO(观察感知残差策略优化)让 Inspector 学会验证据、触发修复 [![[code]](https://img.shields.io/github/stars/Ameame1/PRISIMA)](https://github.com/Ameame1/PRISIMA)
- [2026.01] [[CaRR & C-GRPO]](https://arxiv.org/abs/2601.06021) Chaining the Evidence: Robust Reinforcement Learning for Deep Search Agents with Citation-Aware Rubric Rewards 指出深度搜索 RL 只用二元结果奖励会走捷径、编幻觉，于是把复杂提问拆成可验证的单跳“评分细则”，要求智能体显式补全隐藏实体、给出正确引文并串成完整证据链；再配 C-GRPO 算法联合细则奖励与结果奖励训练，全面抑制捷径，提升证据完备性与事实准确率 [![[code]](https://img.shields.io/github/stars/THUDM/CaRR)](https://github.com/THUDM/CaRR)
- [2026.01] [[TreePS-RAG]](https://arxiv.org/abs/2601.06922) TreePS-RAG: Tree-based Process Supervision for Reinforcement Learning in Agentic RAG 指出仅用最终奖励做 RL 难以对中间推理步骤信用分配，而离线过程监督又易分布漂移；于是把多步检索-推理展开成 rollout 树，节点即步骤，用后代结局的蒙特卡洛估计在线计算每步优势，无需人工中间标签
- [2026.01] [[Dr. Zero]](https://arxiv.org/abs/2601.07055) Dr. Zero: Self-Evolving Search Agents without Training Data 高质量训练数据难获且多轮搜索智能体在“无数据自进化”中问题单一、计算爆炸，于是让同一基模型的“命题者”与“解题者”互搏：命题者不断生成更难却可解的新题，解题者用 hop-grouped 相对策略优化（HRPO）按结构聚类批训，省掉逐题难度评估的采样开销；全程零人工数据，自进化出的智能体在多项任务上追平甚至超越全监督方案 [![[code]](https://img.shields.io/github/stars/facebookresearch/drzero)](https://github.com/facebookresearch/drzero)
- [2026.01] [[RAGShaper]](https://arxiv.org/abs/2601.08699v1) RAGShaper: Eliciting Sophisticated Agentic RAG Skills via Automated Data Synthesis 针对 Agentic RAG 系统缺乏“带噪声”训练数据的痛点，提出 RAGShaper 框架：先用 InfoCurator 围绕种子实体检索并生成感知-认知两级干扰文档，再让教师智能体在“受限导航”下完成多跳问答，自动产出含纠错、抗噪行为的轨迹，无需人工标注即可大规模合成高质量训练数据。
- [2026.01] [[EvoFSM]](https://arxiv.org/abs/2601.09465) EvoFSM: Controllable Self-Evolution for Deep Research with Finite State Machines 把自演化从易失控的“自由改代码”收束到显式有限状态机，将优化空间解耦为宏观 Flow（状态转移）与微观 Skill（状态行为），用批评机制指导少量受控操作迭代 FSM，并配自我演化记忆库，把成功轨迹转为可复用先验、失败模式转为约束 [![[code]](https://img.shields.io/github/stars/QuantaAlpha/EvoFSM)](https://github.com/QuantaAlpha/EvoFSM)
- [2026.01] [[BAPO]](https://arxiv.org/abs/2601.11037) BAPO: Boundary-Aware Policy Optimization for Reliable Agentic Search 用 RL 驱动的搜索智能体虽能迭代检索、提升答案准确率，却普遍缺乏“自知之明”——在证据不足或推理到头时仍强行给出看似合理却不可靠的答案，极少主动回答“我不知道”(IDK)，给高风险场景带来隐患。提出 BAPO（Boundary-Aware Policy Optimization）框架，通过两项机制实现“边界感知”：基于组对比的边界奖励——仅当同组内多条轨迹均无法逼近正确答案时才给 IDK 正向信号，避免误奖；自适应奖励调制器——训练初期暂停该奖励，防止 agent 把 IDK 当捷径滥用。[![[code]](https://img.shields.io/github/stars/Liushiyu-0709/BAPO-Reliable-Search)](https://github.com/Liushiyu-0709/BAPO-Reliable-Search)
- [2026.01] [[Agentic-R]](https://arxiv.org/abs/2601.11888) Agentic-R: Learning to Retrieve for Agentic Search 现有“搜索智能体”多轮检索依赖的仍是面向单轮 RAG 的相似度检索器，无法保证中间 passage 既局部相关又最终导向正确答案，亟需专为多轮 agentic search 定制的检索器。把检索器从“单轮相似”升级为“多轮有用”：先用 LLM 打分衡量局部相关，再用“代入该段落能否推得最终正确答案”衡量全局贡献，自动构建正负例做对比学习；同时让搜索 agent 与检索器双向迭代，agent 产出更高质量查询反哺检索器，两轮后得到跨 agent 通用、EM 提升且减少 10–15% 搜索步数的“自我进化”检索器。 [![[code]](https://img.shields.io/github/stars/8421BCD/Agentic-R)](https://github.com/8421BCD/Agentic-R)
- [2026.01] [[SearchGym]](https://arxiv.org/abs/2601.14615) SearchGym: Bootstrapping Real-World Search Agents via Cost-Effective and High-Fidelity Environment Simulation 通过构建高保真模拟环境解决搜索智能体训练中的数据不对齐问题，用可验证知识图谱和对齐语料库替代昂贵的真实API交互，在此基础上引入SearchGym-RL，一种课程学习方法，通过纯化反馈逐步优化智能体策略，从基本交互发展到复杂的长远规划。 [![[code]](https://img.shields.io/github/stars/JIA-Lab-research/SearchGym)](https://github.com/JIA-Lab-research/SearchGym)
- [2026.01] [[SAGE]](https://arxiv.org/abs/2601.18202v1) SAGE: Steerable Agentic Data Generation for Deep Search with Execution Feedback 深度搜索智能体需要跨文档多跳推理，但人工标注长轨迹成本极高，现有合成数据又难控难度与质量，导致训练样本稀缺且分布失衡。提出 SAGE——可转向的 Agentic 数据生成 pipeline：生成器先草拟 QA → 搜索 agent 实跑轨迹给出“能否答对、难度是否匹配”的执行反馈 → 生成器据此多轮精修问题与答案，直至满足预设难度。 intrinsic 评估显示生成题需多样策略且难度/正确率显著提升；extrinsic 上，用 SAGE 数据训练的 agent 在主流深度搜索基准获最高 23 % 相对提升，并可零样本迁移到 Google 搜索 [![[code]](https://img.shields.io/github/stars/carriex/sage)](https://github.com/carriex/sage)
- [2026.01] [[PaperSearchQA]](https://arxiv.org/abs/2601.18207v1) PaperSearchQA: Learning to Search and Reason over Scientific Papers with RLVR 现有 RLVR 搜索智能体只在通用 QA 上验证“最终答案对不对”，缺乏面向科学文献的深层技术问答，难以满足科研工作者与未来“AI 科学家”的真实需求。如何构建大规模、可验证reward 的科学文献搜索环境，让智能体学会在 1600 万篇生物医学摘要里做复杂检索与推理，并系统评估其规划、自检等能力。发布 PaperSearchQA——含 1600 万摘要的搜索语料 + 6 万可验证事实问答对 + 评测基准；基于 Search-R1 框架训练智能体，以“最终答案 EM”为可验证奖励，显著优于非 RL 检索基线，并展现出规划、推理、自验证等可解释行为；数据与代码全部开源，且创建流程可低成本扩展到其他科学领域 [![[code]](https://img.shields.io/github/stars/jmhb0/PaperSearchQA)](https://github.com/jmhb0/PaperSearchQA) [![Dataset](https://img.shields.io/badge/Dataset-HuggingFace-yellow)](https://huggingface.co/datasets/jmhb/PaperSearchQA)
- [2026.01] [[Dep-Search]](https://arxiv.org/abs/2601.18771v1) Dep-Search: Learning Dependency-Aware Reasoning Traces with Persistent Memory 现有“搜索+推理”框架全靠隐式自然语言串来决策搜什么、怎么用，导致子问题依赖关系混乱、旧知识无法重用、RL 信号稀疏，难以学会最优搜索策略。提出 Dep-Search，用依赖感知的结构化分解将主问题拆成带先后依赖的子图，引入持久记忆库保存已获事实；通过 GRPO 联合优化“何时检索/复用记忆/更新记忆”的显式动作，实现依赖-检索-记忆一体化控制，在 7 个 QA 数据集上显著超越强基线。
- [2026.01] [[ProRAG]](https://arxiv.org/abs/2601.21912v1) ProRAG: Process-Supervised Reinforcement Learning for Retrieval-Augmented Generation 通过MCTS构建过程奖励模型并引入双粒度优势机制，解决了长程多跳RAG任务中基于结果的RL奖励稀疏和信用分配困境，在5个多跳推理基准上显著超越强基线，特别在处理复杂长程任务时表现出优秀的鲁棒性和泛化能力。 [![[code]](https://img.shields.io/github/stars/lilinwz/ProRAG)](https://github.com/lilinwz/ProRAG)
- [2026.01] [[JADE]](https://arxiv.org/abs/2601.21916v1) JADE: Bridging the Strategic-Operational Gap in Dynamic Agentic RAG
  * 动机：现有 Agentic RAG 范式面临关键二分困境：要么在刚性固定图架构内联合优化模块（静态联合优化），丧失动态适应能力；要么赋予动态规划能力却将执行器视为冻结黑盒（动态解耦优化），导致"战略-运营不匹配"——规划器设计的精妙策略因执行器未协同训练而无法实现，反而造成负面性能收益且增加系统复杂度。
    1. 静态方法（如 MMOA-RAG）受限于固定工作流程，无法处理需要多变推理路径的复杂多跳查询；
    2. 解耦方法（如 MAO-ARAG）仅优化规划器而冻结执行器，导致规划与实际执行能力脱节；
    3. 单体方法（如 Search-R1）虽提供端到端灵活性，但缺乏结构先验导致训练不稳定，在巨大上下文窗口中同时学习推理、查询和过滤会陷入优化困境。
  * 提出的方法： JADE（Joint Agentic Dynamic Execution），核心包括：
    1. 参数共享的 MSMDP 建模：将动态 RAG 建模为多智能体半马尔可夫决策过程，规划器和执行器（查询重写、文档选择、答案生成等）共享同一个 LLM 主干，通过角色特定提示区分功能；
    2. 统一经验回放缓冲：将异构的规划和执行转移数据聚合到共享缓冲区，使用 PPO 进行端到端联合优化；
    3. 双层奖励机制：全局共享奖励（最终答案质量减去计算成本）促进团队协作解决信用分配问题，局部格式惩罚确保各角色输出结构合规；
    4. 动态工作流编排：规划器根据查询复杂度自适应选择"串行分解"、"并行分解"或"直接求解"等工作流拓扑。
- [2026.01] [[DeepSearchQA]](https://arxiv.org/abs/2601.20975) DeepSearchQA: Bridging the Comprehensiveness Gap for Deep Research Agents 通过引入 900 个要求生成完整答案集的多步骤检索任务，解决了现有单答案基准无法评估的"全面性缺口"问题（包括系统整理、实体去重和停止准则推理），采用 F1 分数等严格指标对深度研究智能体进行基于结果的评估，揭示了即使最先进的 Gemini Deep Research Agent 和 GPT-5 Pro 在平衡召回率与精确率（F1 约 81-82%）及完全正确率（约 66%）方面仍存在显著局限性。 [![Dataset](https://img.shields.io/badge/Dataset-Kaggle-blue)](https://www.kaggle.com/benchmarks/google/dsqa/leaderboard)
- [2026.02] [[InfoReasoner]](https://arxiv.org/abs/2602.00845) Optimizing Agentic Reasoning with Retrieval via Synthetic Semantic Information Gain Reward 针对智能体推理中检索优化缺乏密集奖励信号的问题，提出了一种基于合成语义信息增益奖励的统一框架；理论上将信息增益重新定义为模型信念状态的不确定性减少，实践上通过双向文本蕴含的语义聚类设计输出感知的内在估计器，无需人工标注即可直接从输出分布计算奖励 [![[code]](https://img.shields.io/github/stars/dl-m9/InfoReasoner)](https://github.com/dl-m9/InfoReasoner)
- [2026.02] [[RE-TRAC]](https://arxiv.org/abs/2602.02486) RE-TRAC: REcursive TRAjectory Compression for Deep Search Agents 针对当前基于大语言模型（LLM）的深度研究代理在执行复杂信息检索任务时存在的局限性展开，动机源于主流的ReAct框架采用线性推理结构，难以回溯先前状态、探索替代路径或在长上下文下保持全局感知，容易陷入局部最优、重复探索和低效搜索。要解决的核心问题是如何突破ReAct框架的线性限制，实现跨轨迹的有效探索与全局感知，以提升深度研究代理的信息收集与推理效率。为解决该问题，作者提出了**RE-TRAC**（REcursive TRAjectory Compression），一种通过在每条搜索轨迹后生成结构化状态表示来总结证据、不确定性、失败原因及未来计划，并将后续轨迹建立在此状态表示之上的新型代理框架，从而支持迭代反思和全局引导的规划。该方法通过轨迹压缩机制实现递归状态更新，使研究过程更具渐进性和目标导向性。 [![[code]](https://img.shields.io/github/stars/microsoft/InfoAgent)](https://github.com/microsoft/InfoAgent)
- [2026.02] [[DAS]](https://arxiv.org/abs/2602.03304v1) To Search or Not to Search: Aligning the Decision Boundary of Deep Search Agents via Causal Intervention 如何诊断和纠正深度搜索智能体的决策边界错位问题（包括过度搜索和搜索不足），即确定何时积累的信息足以回答的阈值；提出DAS（Decision Boundary Alignment for Deep Search agents）框架，包含两个关键组件：一是基于因果干预的诊断方法，通过在每个决策点比较事实轨迹和反事实轨迹来识别边界错误；二是构建来自因果反馈的偏好数据集并通过偏好优化对齐策略，从而有效校准决策边界，缓解过度搜索和搜索不足问题。 [![[code]](https://img.shields.io/github/stars/Applied-Machine-Learning-Lab/WWW2026_DAS)](https://github.com/Applied-Machine-Learning-Lab/WWW2026_DAS)
- [2026.02] [[A-RAG]](https://arxiv.org/abs/2602.03442v1) A-RAG: Scaling Agentic Retrieval-Augmented Generation via Hierarchical Retrieval Interfaces 现有RAG系统仍依赖两种范式（单次检索后拼接输入或预定义工作流逐步执行），这些范式不允许模型参与检索决策，无法随着模型能力提升而有效扩展；提出A-RAG框架，通过将分层检索接口直接暴露给模型，提供关键词搜索、语义搜索和块读取三种检索工具，使智能体能够在多个粒度上自适应地搜索和检索信息，从而有效利用模型能力并动态适应不同RAG任务。 [![[code]](https://img.shields.io/github/stars/Ayanami0730/arag)](https://github.com/Ayanami0730/arag)
- [2026.02] [[Search-R2]](https://arxiv.org/abs/2602.03647v1) Search-R2: Enhancing Search-Integrated Reasoning via Actor-Refiner Collaboration 搜索集成推理使语言智能体能够超越静态参数知识主动查询外部信息源，但通过强化学习训练这些智能体时面临多尺度信用分配问题（现有方法依赖稀疏的轨迹级奖励，无法区分高质量推理和偶然猜测，导致冗余或误导性的搜索行为）；提出Search-R2框架，通过Actor-Refiner协作机制增强推理，其中Actor生成初始推理轨迹，Meta-Refiner通过"剪切-再生"机制选择性诊断和修复缺陷步骤，并引入结合结果正确性与证据信息密度的混合奖励设计，将Actor-Refiner交互形式化为平滑混合策略，证明选择性修正能严格优于强基线。
- [2026.02] [[BAR-RAG]](https://arxiv.org/abs/2602.03689v1) Rethinking the Reranker: Boundary-Aware Evidence Selection for Robust Retrieval-Augmented Generation 检索增强生成（RAG）系统依赖重排序器（reranker）从检索到的文档中选择最相关的证据，但现有方法忽视了检索文档与查询之间的边界关系，导致在证据选择时无法有效区分真正相关的文档和噪声文档；提出边界感知证据选择方法（Boundary-Aware Evidence Selection），通过引入边界感知机制来显式建模文档与查询之间的边界关系，实现更鲁棒的证据选择，提升检索增强生成系统在存在噪声文档时的性能。Reranker变成Selector。 [![[code]](https://img.shields.io/github/stars/GasolSun36/BAR-RAG)](https://github.com/GasolSun36/BAR-RAG)
- [2026.02] [[BranPO]](https://arxiv.org/abs/2602.03719v1) Training Multi-Turn Search Agent via Contrastive Dynamic Branch Sampling 现有的智能体强化学习在长周期多轮任务中面临稀疏轨迹级奖励带来的学习困难，而基于树的方法虽然试图缓解此问题但存在高方差和计算效率低下的缺陷，作者通过经验分析发现性能差异主要由轨迹尾部附近的决策造成；提出分支相对策略优化（BranPO），通过截断轨迹尾部并重新采样替代延续来构建共享前缀上的对比后缀，结合难度感知分支采样和冗余步骤掩码技术，实现了无需价值模型的高效稳定训练。 [![[code]](https://img.shields.io/github/stars/YubaoZhao/BranPO)](https://github.com/YubaoZhao/BranPO)
- [2026.02] [[DeepRead]](https://arxiv.org/abs/2602.05014v1) DeepRead: Document Structure-Aware Reasoning to Enhance Agentic Search 现有的智能体搜索系统（如Search-o1-style）在处理文档问答任务时，未能充分利用文档的结构信息，导致在理解和推理复杂文档内容时效率低下；DeepRead框架，通过引入文档结构感知的推理机制来增强智能体搜索能力，使系统能够理解和利用文档的层次结构、章节关系等结构信息，从而显著提升文档问答任务的性能。
- [2026.02] [[SAGE]](https://arxiv.org/abs/2602.05975) SAGE: Benchmarking and Improving Retrieval for Deep Research Agents 现有深度研究智能体（deep research agents）在复杂信息寻求任务中依赖检索系统，但缺乏系统性的研究来深入理解这些智能体的检索行为，特别是在需要深度推理的科学文献搜索任务中；SAGE（Scientific AGentic retrieval Evaluation）基准测试框架，包含1,200个查询和20万篇论文的科学文献语料库，涵盖四种科学领域，同时提出一种语料库级别的测试时扩展框架，通过利用LLM推理为每篇论文生成信息丰富的元数据和关键词来丰富语料库，使现成的检索器更容易找到相关论文。 [![[code]](https://img.shields.io/github/stars/HughieHu/Sage)](https://github.com/HughieHu/Sage)
- [2026.02] [[SCOUT-RAG]](https://arxiv.org/abs/2602.08400v1) SCOUT-RAG: Scalable and Cost-Efficient Unifying Traversal for Agentic Graph-RAG over Distributed Domains 传统 Graph-RAG（基于知识图谱的检索增强生成）依赖集中式知识图谱设计，但在以下场景面临挑战：分布式环境：数据分散在不同领域（如医院、跨国组织）；访问受限：无法获得全局图谱可见性；成本敏感：无法承受穷举式查询的高昂开销。需要在无全局可见性和避免穷举查询的约束下，智能选择相关领域并确定合适的遍历深度。渐进式跨领域检索：基于增量效用目标引导检索过程，而非一次性穷举；四智能体协作系统	① 领域相关性估计 — 判断哪些领域可能包含相关信息 ② 扩展决策 — 决定何时扩展到额外领域 ③ 深度自适应 — 动态调整遍历深度，避免不必要的图谱探索 ④ 答案合成 — 整合检索结果生成高质量答案
- [2026.02] [[GISA]](https://arxiv.org/abs/2602.08543v1) GISA: A Benchmark for General Information-Seeking Assistant 本研究旨在解决现有信息检索代理评估基准中存在的诸多问题，如任务设计不贴近真实用户需求、依赖反向构建查询导致的不自然性、以及静态答案集易受数据污染等；为实现这一目标，GISA采用了由人工精心设计的373个真实信息寻求查询，并引入四种结构化答案格式（项目、集合、列表和表格），支持确定性度量评估；同时构建了一个包含完整人类搜索轨迹的数据集，可用于过程级监督与模仿学习，并设立定期更新的动态子集以抵抗模型记忆 [![[code]](https://img.shields.io/github/stars/RUC-NLPIR/GISA)](https://github.com/RUC-NLPIR/GISA)
